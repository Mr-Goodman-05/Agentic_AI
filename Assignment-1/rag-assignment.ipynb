{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23475e2e-0c65-4607-8897-1f38c456075b",
   "metadata": {},
   "source": [
    "# RAG Assignment — Jupyter Notebook\n",
    "**Author:** Jafar Alsaleh \n",
    "**Purpose:** Build a Retrieval-Augmented Generation (RAG) pipeline that uses PDF-only knowledge sources, FAISS vector DB, sentence-transformer embeddings, and a generation model. The notebook contains: extraction, chunking, embedding, FAISS indexing, retrieval, and generation. It includes at least 3 test queries with outputs.\n",
    "\n",
    "**Files required (place in notebook directory):**\n",
    "- `alice_in_wonderland.pdf`\n",
    "- `pride_and_prejudice.pdf`\n",
    "- `sherlock_adventures.pdf`\n",
    "\n",
    "Run each cell in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82813e-1153-4b91-9fdb-7be9705aee26",
   "metadata": {},
   "source": [
    "## 0) Environment / Install dependencies\n",
    "\n",
    "This cell installs all required packages. If you are in a constrained environment (e.g., Windows where `faiss-cpu` pip wheels may not exist), follow the troubleshooting notes below the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca58e74-6420-4a8f-989c-9c6865a8a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q pdfplumber sentence-transformers faiss-cpu transformers torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e849eee-df15-499f-9e92-44defae6f384",
   "metadata": {},
   "source": [
    "## 1) Imports and helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c4fa22-5e45-46cb-b94c-81aad61adf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Generation fallback: transformers pipeline (FLAN-T5 small) if OPENAI not used.\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Helper: simple text cleanup\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace('\\r', '\\n')\n",
    "    s = re.sub(r'\\n\\s*\\n+', '\\n\\n', s)   # collapse multiple blank lines\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "# Helper: save/load index + metadata\n",
    "def save_faiss_index(index, embeddings_meta, index_path=\"faiss_index.bin\", meta_path=\"index_meta.pkl\"):\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(meta_path, \"wb\") as f:\n",
    "        pickle.dump(embeddings_meta, f)\n",
    "\n",
    "def load_faiss_index(index_path=\"faiss_index.bin\", meta_path=\"index_meta.pkl\"):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    return index, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd49608-23e1-44b2-930a-4cb264d54bd7",
   "metadata": {},
   "source": [
    "## 2) Problem statement\n",
    "\n",
    "**Purpose of the system:**  \n",
    "Create a Retrieval-Augmented Generation system that answers natural language queries by retrieving relevant text chunks from a set of text-only PDF documents (public or self-created), using dense vector embeddings and FAISS for fast similarity search, then conditions a text generator on the retrieved passages to produce accurate, grounded answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e618c-b2e6-43ab-9a88-71be03c72761",
   "metadata": {},
   "source": [
    "## 3) Dataset / Knowledge Source\n",
    "\n",
    "- **File type:** PDF only  \n",
    "- **Content type:** Text only (no scanned images; PDFs should contain selectable text)  \n",
    "- **Source:** Public domain PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8be094-2d26-4c96-9228-082d9a9cddfe",
   "metadata": {},
   "source": [
    "## 4) RAG Architecture — block diagram\n",
    "\n",
    "Below is a textual/block diagram showing the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9dce31-ea6d-4223-8748-f9b27881f642",
   "metadata": {},
   "source": [
    "PDF Collection (3 PDFs) ---> Text Extraction (pdfplumber) ---> Chunking & Index (overlapping) ---> Context Assembly (top-k chunks + prompt) ---> Generator (OpenAI or local FLAN-T5) ---> Final Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428faadc-52fe-43f8-8a6f-9d8117e5a279",
   "metadata": {},
   "source": [
    "## 5) Text chunking strategy\n",
    "\n",
    "We use **character-based chunking** with overlap:\n",
    "\n",
    "- **Chunk size:** 1000 characters (~150–220 words typical)  \n",
    "- **Chunk overlap:** 200 characters\n",
    "\n",
    "**Reasoning / justification:**\n",
    "- Keeps chunks large enough to retain short narrative sections or paragraphs (context) so the generator sees coherent text.\n",
    "- Overlap of 200 chars reduces boundary-cut issues (important so sentences/concepts that cross chunk borders are not lost).\n",
    "- Character-based chunking is robust across fonts/models where tokenizers might differ; it's implementation-simple and deterministic.\n",
    "- For production, you may adapt to token-based chunking (e.g., 512–1024 tokens) if using a specific LLM tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80713aa2-946d-40f3-98f9-ba0bca38b3f4",
   "metadata": {},
   "source": [
    "## 6) Embedding model selection\n",
    "\n",
    "**Model used:** `all-MiniLM-L6-v2` from SentenceTransformers.\n",
    "\n",
    "**Why chosen:**\n",
    "- Small, fast, and accurate for semantic search over varied text.\n",
    "- Produces 384-dimensional dense vectors — compact for FAISS.\n",
    "- No API keys required (runs locally with CPU/GPU).\n",
    "- Widely used in RAG/semantic search tutorials; good tradeoff between performance and compute needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3089aa27-8a0b-403b-b8d0-4a425b56dd9d",
   "metadata": {},
   "source": [
    "## 7) Implementation: Data-loading → Chunking → Embedding → FAISS → Retrieval → Generation\n",
    "\n",
    "Full code block-by-block. Read comments. Run sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946eaadd-1497-4c5b-9611-dc6d15a63fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded alice_in_wonderland.pdf: 142706 characters\n",
      "Loaded pride_and_prejudice.pdf: 505894 characters\n",
      "Loaded sherlock_adventures.pdf: 587310 characters\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Load PDFs and extract text\n",
    "pdf_files = [\"alice_in_wonderland.pdf\", \"pride_and_prejudice.pdf\", \"sherlock_adventures.pdf\"]\n",
    "\n",
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    text_pages = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            txt = p.extract_text()\n",
    "            if txt:\n",
    "                text_pages.append(txt)\n",
    "\n",
    "    full_text = clean_text(\"\\n\\n\".join(text_pages))\n",
    "\n",
    "    # Remove Project Gutenberg boilerplate\n",
    "    start_marker = \"*** START OF\"\n",
    "    end_marker = \"*** END OF\"\n",
    "\n",
    "    start_idx = full_text.find(start_marker)\n",
    "    if start_idx != -1:\n",
    "        full_text = full_text[start_idx + 200:]  # skip header safely\n",
    "\n",
    "    end_idx = full_text.find(end_marker)\n",
    "    if end_idx != -1:\n",
    "        full_text = full_text[:end_idx]\n",
    "\n",
    "    return full_text.strip()\n",
    "\n",
    "\n",
    "corpus_texts = {}\n",
    "for f in pdf_files:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"Required file not found: {f}. Please upload it.\")\n",
    "    corpus_texts[f] = extract_text_from_pdf(f)\n",
    "    print(f\"Loaded {f}: {len(corpus_texts[f])} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e51335-644b-433a-98fd-78595ec4879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1547\n"
     ]
    }
   ],
   "source": [
    "# 7.2 Chunking function (character-wise)\n",
    "def chunk_text(text: str, chunk_size: int = 2000, chunk_overlap: int = 300) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    while start < L:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(clean_text(chunk))\n",
    "        start = end - chunk_overlap  # overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return [c for c in chunks if len(c) > 50]  # drop tiny chunks\n",
    "\n",
    "# Create chunk list with metadata\n",
    "all_chunks = []\n",
    "for fname, txt in corpus_texts.items():\n",
    "    cks = chunk_text(txt, chunk_size=1000, chunk_overlap=200)\n",
    "    for i, c in enumerate(cks):\n",
    "        meta = {\"source\": fname, \"chunk_id\": f\"{os.path.basename(fname)}_chunk_{i}\", \"text_preview\": c[:200]}\n",
    "        all_chunks.append((c, meta))\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcab6d54-619f-49f4-a1b6-d92f517a70e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c36afbccea400281dae11242019c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1547, 384)\n"
     ]
    }
   ],
   "source": [
    "# 7.3 Embeddings — model load and encode all chunks\n",
    "embed_model_name = \"all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(embed_model_name)\n",
    "\n",
    "# Batch encode\n",
    "texts = [c for c, _ in all_chunks]\n",
    "batch_size = 64\n",
    "embeddings = embed_model.encode(texts, show_progress_bar=True, batch_size=batch_size, convert_to_numpy=True)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9eee4a5-a5d2-42ec-85e1-cfaecd12942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index size (n): 1547\n",
      "Index and metadata saved.\n"
     ]
    }
   ],
   "source": [
    "# 7.4 Build FAISS index (inner-product / cosine similarity via normalization)\n",
    "d = embeddings.shape[1]\n",
    "# We'll use IndexFlatIP with normalized vectors for cosine similarity\n",
    "index = faiss.IndexFlatIP(d)\n",
    "# normalize embeddings for cosine similarity\n",
    "def normalize(v: np.ndarray):\n",
    "    norms = np.linalg.norm(v, axis=1, keepdims=True)\n",
    "    norms[norms==0] = 1e-10\n",
    "    return v / norms\n",
    "\n",
    "embeddings_norm = normalize(embeddings.astype('float32'))\n",
    "index.add(embeddings_norm)\n",
    "print(\"FAISS index size (n):\", index.ntotal)\n",
    "\n",
    "# Save metadata mapping: index->meta\n",
    "index_meta = [meta for _, meta in all_chunks]\n",
    "# Save index and meta\n",
    "save_faiss_index(index, index_meta, index_path=\"faiss_index.bin\", meta_path=\"index_meta.pkl\")\n",
    "print(\"Index and metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd6c1a9-4fdd-4fd8-b25e-7c8c928cdba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.601995587348938 source: alice_in_wonderland.pdf\n",
      "d neither of the others took the least notice of her going, though she looked back once or twice, half hoping that they would call after her: the last time she saw them, they were trying to put the Dormouse into the teapot. 'At any rate I'll never go THERE again!' said Alice as she picked her way through the wood. 'It's the stupidest tea-party I ever was at in all my life!' Just as she said this, \n",
      "----\n",
      "score: 0.5658360719680786 source: alice_in_wonderland.pdf\n",
      "eeble voice: 'I heard every word you fellows were saying.' 'Tell us a story!' said the March Hare. 'Yes, please do!' pleaded Alice. 'And be quick about it,' added the Hatter, 'or you'll be asleep again before it's done.' 'Once upon a time there were three little sisters,' the Dormouse began in a great hurry; 'and their names were Elsie, Lacie, and Tillie; and they lived at the bottom of a well—' '\n",
      "----\n",
      "score: 0.562087893486023 source: alice_in_wonderland.pdf\n",
      "tale, perhaps even with the dream of Wonderland of long ago: and how she would feel with all their simple sorrows, and find a pleasure in all their simple joys, remembering her own child-life, and the happy summer days. THE END End of Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# 7.5 Retrieval function: return top_k chunks for a query\n",
    "def embed_query(query: str):\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "    return normalize(q_emb.astype('float32'))\n",
    "\n",
    "def retrieve(query: str, top_k: int = 5):\n",
    "    qv = embed_query(query)\n",
    "    D, I = index.search(qv, top_k)  # D=similarity scores, I=indices\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        meta = index_meta[idx]\n",
    "        chunk_text = texts[idx]\n",
    "        results.append({\"score\": float(score), \"meta\": meta, \"text\": chunk_text})\n",
    "    return results\n",
    "\n",
    "# Quick local test (no generation) to see retrieval\n",
    "sample_q = \"Who is Alice and how does she enter Wonderland?\"\n",
    "retrieved = retrieve(sample_q, top_k=3)\n",
    "for r in retrieved:\n",
    "    print(\"score:\", r['score'], \"source:\", r['meta']['source'])\n",
    "    print(r['text'][:400].replace(\"\\n\",\" \"))\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5643a8a-aa89-44bd-b4b0-6e78d06c6d91",
   "metadata": {},
   "source": [
    "## 8) Generation layer (two options)\n",
    "\n",
    "- **Option A (recommended if you have OpenAI API key):** Use OpenAI's completion API (gpt-3.5 / gpt-4) to condition on retrieved chunks. (Put your `OPENAI_API_KEY` in environment or use your preferred method.)\n",
    "- **Option B (local, API-free):** Use HuggingFace **FLAN-T5 small** local generator (`google/flan-t5-small`) via `transformers`. This is slower but offline. We'll include both; the notebook will automatically pick OpenAI if you set `OPENAI_API_KEY`, otherwise it will use FLAN-T5 small.\n",
    "\n",
    "**Note:** If you use Flan-T5 small, quality will be lower than a modern OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c220e47e-5e06-484f-9128-71713d50c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Prepare a simple context-assembly & prompt template\n",
    "def assemble_context(retrieved_chunks: List[dict], max_chars: int = 1200):\n",
    "    ctx_parts = []\n",
    "    total = 0\n",
    "\n",
    "    for r in retrieved_chunks:\n",
    "        t = r['text']\n",
    "\n",
    "        if total + len(t) > max_chars:\n",
    "            remaining = max_chars - total\n",
    "            if remaining > 100:\n",
    "                ctx_parts.append(t[:remaining])\n",
    "            break\n",
    "\n",
    "        ctx_parts.append(t)\n",
    "        total += len(t)\n",
    "\n",
    "    return \"\\n\\n\".join(ctx_parts)\n",
    "\n",
    "\n",
    "def make_prompt(query: str, retrieved_chunks: List[dict]):\n",
    "    context = assemble_context(retrieved_chunks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant.\n",
    "\n",
    "Using ONLY the provided context, write a short summarized answer in 3-5 sentences.\n",
    "Do NOT copy large parts of the text.\n",
    "Explain clearly in your own words.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Final Answer:\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea03e4ac-c81d-4ab9-9508-7948faea74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local generator (flan-t5-small) loaded.\n"
     ]
    }
   ],
   "source": [
    "# 8.2 Option B (local) — load FLAN-T5 pipeline (fallback)\n",
    "# We will use this if OPENAI_API_KEY is not set.\n",
    "try:\n",
    "    generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device=-1, max_length=384)\n",
    "    print(\"Local generator (flan-t5-small) loaded.\")\n",
    "except Exception as e:\n",
    "    print(\"Local generator failed to load. You can use OpenAI option by setting OPENAI_API_KEY. Error:\", e)\n",
    "    generator = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10130f78-8575-4e83-a2b6-2d8ba08696c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Generation function wrapper: tries OpenAI first, else uses local generator\n",
    "import os\n",
    "\n",
    "def generate_answer(prompt: str, use_openai: bool = False, openai_model: str = \"gpt-3.5-turbo\"):\n",
    "    # If user has OPENAI_API_KEY and use_openai True, call OpenAI.\n",
    "    if use_openai and os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        try:\n",
    "            import openai\n",
    "            openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "            # Chat completion\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=openai_model,\n",
    "                messages=[{\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "                          {\"role\":\"user\",\"content\":prompt}],\n",
    "                max_tokens=400,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI generate failed:\", e)\n",
    "            # fall back to local generator\n",
    "    # fallback: local generator\n",
    "    if generator:\n",
    "        out = generator(prompt, max_length=384, do_sample=False)\n",
    "        return out[0]['generated_text'].strip()\n",
    "    else:\n",
    "        raise RuntimeError(\"No generator available. Set OPENAI_API_KEY or install the local model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445d219-e6fa-4aab-9ba3-4534f339697f",
   "metadata": {},
   "source": [
    "## 9) Retrieval + Generation function (end-to-end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13329757-9b92-4b05-a5f9-757ca1a72c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str, top_k: int = 5, use_openai_if_available: bool = False):\n",
    "    retrieved = retrieve(query, top_k=top_k)\n",
    "    prompt = make_prompt(query, retrieved)\n",
    "    answer = generate_answer(prompt, use_openai=use_openai_if_available)\n",
    "    # Provide sources seen\n",
    "    sources = list({r['meta']['source'] for r in retrieved})\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"retrieved\": retrieved\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ec8aa-68a9-4a23-973b-6e62d44d7e30",
   "metadata": {},
   "source": [
    "# 10) Minimum 3 test queries with outputs.\n",
    "#### We'll show the query, execution code, and **expected** outputs (examples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf3e1f10-eebb-457c-9e6d-99817d4f2441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUERY: Who is Alice and how does she enter Wonderland? Summarize briefly and reference the source.\n",
      "\n",
      "--- GENERATED ANSWER ---\n",
      "\n",
      "The END End of Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll d neither of the others took the least notice of her going, though she looked back once or twice, half hoping that they would call after her: the last time she saw them, they were trying to put the Dormouse into the teapot. 'At any rate I'll never go THERE again!' said Alice as she picked her way through the wood. 'It's the stupidest tea-party I ever was at in all my life!'\n",
      "\n",
      "--- SOURCES (retrieved) ---\n",
      "['alice_in_wonderland.pdf']\n",
      "\n",
      "--- TOP retrieved chunk previews ---\n",
      "[0] score=0.5571 source=alice_in_wonderland.pdf preview=tale, perhaps even with the dream of Wonderland of long ago: and how she would feel with all their simple sorrows, and find a pleasure in all their simple joys, remembering her own child-life, and the\n",
      "[1] score=0.5427 source=alice_in_wonderland.pdf preview=d neither of the others took the least notice of her going, though she looked back once or twice, half hoping that they would call after her: the last time she saw them, they were trying to put the Do\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUERY: What is Elizabeth Bennet's view about marriage and how does it differ from other characters?\n",
      "\n",
      "--- GENERATED ANSWER ---\n",
      "\n",
      "Elizabeth's husband regarded the affair as she wished.\n",
      "\n",
      "--- SOURCES (retrieved) ---\n",
      "['pride_and_prejudice.pdf']\n",
      "\n",
      "--- TOP retrieved chunk previews ---\n",
      "[0] score=0.6618 source=pride_and_prejudice.pdf preview=42-h.htm 79/147  17/02/2026, 00:18 Pride and prejudice | Project Gutenberg Elizabeth could not but smile at such a conclusion of such a beginning; but Mrs. Bennet, who had persuaded herself that her h\n",
      "[1] score=0.6484 source=pride_and_prejudice.pdf preview=ear my opinion.” Mrs. Bennet rang the bell, and Miss Elizabeth was summoned to the library. “Come here, child,” cried her father as she appeared. “I have sent for you on an affair of importance. I und\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUERY: In 'A Scandal in Bohemia', who is the principal character, and what is the main plot/issue?\n",
      "\n",
      "--- GENERATED ANSWER ---\n",
      "\n",
      "A Scandal in Bohemia is the main character of the 'A Scandal in Bohemia'.\n",
      "\n",
      "--- SOURCES (retrieved) ---\n",
      "['sherlock_adventures.pdf']\n",
      "\n",
      "--- TOP retrieved chunk previews ---\n",
      "[0] score=0.5290 source=sherlock_adventures.pdf preview=” said he. “You may say before this gentleman anything which you may say to me.” The Count shrugged his broad shoulders. “Then I must begin,” said he, “by binding you both to absolute secrecy for two \n",
      "[1] score=0.5139 source=sherlock_adventures.pdf preview=in, a fair proportion do not treat of crime, in its legal sense, at all. The small matter in which I endeavoured to help the King of Bohemia, the singular experience of Miss Mary Sutherland, the probl\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test queries (you can change them)\n",
    "test_queries = [\n",
    "    # Query for Alice\n",
    "    \"Who is Alice and how does she enter Wonderland? Summarize briefly and reference the source.\",\n",
    "    # Query for Pride & Prejudice\n",
    "    \"What is Elizabeth Bennet's view about marriage and how does it differ from other characters?\",\n",
    "    # Query for Sherlock\n",
    "    \"In 'A Scandal in Bohemia', who is the principal character, and what is the main plot/issue?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for q in test_queries:\n",
    "    print(\"=\"*80)\n",
    "    print(\"QUERY:\", q)\n",
    "    res = answer_query(q, top_k=2, use_openai_if_available=False)  # set True if you have OPENAI_API_KEY and want to use it\n",
    "    print(\"\\n--- GENERATED ANSWER ---\\n\")\n",
    "    print(res['answer'])\n",
    "    print(\"\\n--- SOURCES (retrieved) ---\")\n",
    "    print(res['sources'])\n",
    "    print(\"\\n--- TOP retrieved chunk previews ---\")\n",
    "    for i, r in enumerate(res['retrieved']):\n",
    "        print(f\"[{i}] score={r['score']:.4f} source={r['meta']['source']} preview={r['meta']['text_preview'][:200].replace(chr(10),' ')}\")\n",
    "    print(\"\\n\")\n",
    "    results.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af04b7a-f3b0-4b79-a0fb-6cf6d68abfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
